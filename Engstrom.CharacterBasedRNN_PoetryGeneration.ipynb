{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "146b8217",
   "metadata": {},
   "source": [
    "## Vanilla RNN for poetry generation \n",
    "#### RNN contains one hidden layer with 101 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "269c0829",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code written with guidance of tutorial by Andrej Karpathy, https://gist.github.com/karpathy/d4dee566867f8291f086"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16156a1",
   "metadata": {},
   "source": [
    "## Part 1: define functions / initialize variables / set hyperparmaters "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92e34ad",
   "metadata": {},
   "source": [
    "#### 1. Import dataset. Derive text and concatenate into a text file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "id": "6db9092c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data in MB 0.713912\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "poems_df = pd.read_csv('poems.csv',sep=\",\",encoding='unicode_escape')\n",
    "\n",
    "\n",
    "#unfortunately we have to concatenate the whole dataset to get it as close as possible to 1MB (all poems together only .75MB)\n",
    "#isolate renaissance poems and concatenate contents \n",
    "poems = poems_df['content']\n",
    "all_poems = \"\"\n",
    "for p in range(poems.shape[0]):\n",
    "    all_poems = all_poems + str(poems[p])\n",
    "\n",
    "characters = list(set(all_poems))\n",
    "\n",
    "#size of data in total characters and size of vocabulary in unique characters\n",
    "sz_char = len(characters)\n",
    "sz_dat = len(all_poems)\n",
    "sz_mb = sz_dat/1000000\n",
    "print(\"data in MB\", sz_mb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06661e6c",
   "metadata": {},
   "source": [
    "#### 2. Define model hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "id": "d431adb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters \n",
    "hidden_neurons = sz_char # the number of neurons in the hidden layer should be roughly the mean between the number of input neurons and the number of output neurons. Both are equal in this case, and 101 \n",
    "no_steps_unroll = 25 # the number of steps to unroll the RNN for; should be equivalent to the number of input unity \n",
    "learning_rate = .1 #experiment with this? steps of .1 between 1 and .001? \n",
    "\n",
    "#model parameters \n",
    "\n",
    "#randomly initialize weight matrices (sample from gaussian dist with var of 0.01). Should have the dimenions of the two layers being connected \n",
    "#'h' is hidden layer\n",
    "#'i' is input layer\n",
    "#'o' is output layer\n",
    "W_ih = np.random.standard_normal(size=(hidden_neurons,sz_char))*.01 #connecting input to hidden layer\n",
    "W_hh =  np.random.standard_normal(size=(hidden_neurons,hidden_neurons))*.01 #connecting hidden to hidden layers\n",
    "W_ho = np.random.standard_normal(size=(sz_char,hidden_neurons))*.01 #connecting hidden to output layer\n",
    "\n",
    "#initialize bias terms (intercept) as 0s for hidden and output layers\n",
    "b_h = np.zeros((hidden_neurons,1))\n",
    "b_o = np.zeros((sz_char,1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1c1d7f",
   "metadata": {},
   "source": [
    "#### 3. Define loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "0e27f23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy as copy\n",
    "import math\n",
    "\n",
    "#loss function should take as arguments: input, 'targets', as well as the initial states of the hidden layers \n",
    "#inputs -> a list of integers where each integer represents a character\n",
    "#targets -> a list of integers where each integer represents a ____ \n",
    "def loss_iterate(inputs,targets,initial_hidden,clip_params):\n",
    "    #set loss to zero \n",
    "    loss = 0\n",
    "    \n",
    "    #initialize dictionaries for inputs, hidden states, predictions, targets \n",
    "    ins = {} #each instance is a Kx1 vector where k = the length of the vocabulary (sz_char)\n",
    "    hidds = {} #each instance is a Kx1 vector where k = the length of the vocabulary (sz_char)\n",
    "    outs = {} #unnormalized log probabilities. \n",
    "    probs = {} #normalized probabilities.  \n",
    "    \n",
    "    #specify initial hidden state (stored under -1 key). This is so t-1 in first forward pass (index=0) returns this vector\n",
    "    hidds[-1] = np.copy(initial_hidden)\n",
    "    \n",
    "    #forward pass to generate predictions about next character \n",
    "    for c in range(len(inputs)):\n",
    "        #one hot encoding of vector representing input (where 1 is in position of character representation, 0 in all other positions)\n",
    "        ins[c] = np.zeros((sz_char,1)) \n",
    "        ins[c][inputs[c]] = 1\n",
    "        #new hidden state (weighted inputs + weighted[previous] hidden state + bias, squashed between 0 and 1)\n",
    "        hidds[c] = np.tanh(np.dot(W_ih,ins[c]) + np.dot(W_hh,hidds[c-1]) + b_h)\n",
    "        #print(hidds[c].size)\n",
    "        #generate unnormalized log probabilities \n",
    "        outs[c] = np.dot(W_ho, hidds[c]) + b_o\n",
    "        #print(outs[c].size)\n",
    "        #normalize these probabilities so that they are non-negative and so that they sum to 1\n",
    "        probs[c] = np.exp(outs[c]) / np.sum(np.exp(outs[c]))\n",
    "        #compute cross-entropy loss (softmax). Note that the probability vector is expected to be XXXXXXXX. hence, we specify first column.\n",
    "        loss = loss + -np.log(probs[c][targets[c],0])\n",
    "    \n",
    "    #compute gradients \n",
    "    #weight matrices \n",
    "    dW_ih = np.zeros((hidden_neurons,sz_char))\n",
    "    dW_hh = np.zeros((hidden_neurons,hidden_neurons))\n",
    "    dW_ho = np.zeros((sz_char,hidden_neurons))\n",
    "    #biases\n",
    "    d_b_out = np.zeros_like(b_o)\n",
    "    d_b_hid = np.zeros_like(b_h)\n",
    "    #next hidden state between iterations\n",
    "    dhidd_next = np.zeros_like(hidds[0])\n",
    "    \n",
    "    #this process is carried out backwards (from end to beginning of forward pass) to compare target predictions of next character with target and use gradients to update params of previous cells \n",
    "    for c in reversed(range(len(inputs))):\n",
    "        #update probs by minusing 1 (this is because of the derivative of negative log of the current probabilities, which would return us to the original unnormalized probability estimates)\n",
    "        probs_up = np.copy(probs[c])\n",
    "        probs_up[targets[c]] = probs_up[targets[c]] - 1 \n",
    "        #determining updates for weights (hidden-output layer) based on probability gradients \n",
    "        dW_ho = dW_ho + np.dot(probs_up,hidds[c].transpose())\n",
    "        #determining updates for bias (from hidden to output layer) \n",
    "        d_b_out = d_b_out + probs_up\n",
    "        #determining updates to hidden layer (POST tanh function)\n",
    "        hidds_up = np.dot(W_ho.transpose(),probs_up) + dhidd_next \n",
    "        #determining updates to hidden layer (PRE tanh function). note derivative of tanh function = 1 - x**2, where x is hidden state in previous iteration\n",
    "        hidds_up_unnorm = (1-hidds[c]*hidds[c]) * hidds_up\n",
    "        #determining updates for bias (from hidden to output layer) \n",
    "        d_b_hid = d_b_hid + hidds_up_unnorm\n",
    "        #determining updates for weights (hidden-hidden layer)\n",
    "        dW_hh = dW_hh + np.dot(hidds_up_unnorm,hidds[c-1].transpose())\n",
    "        #determining updates for weights (hidden-output layer)\n",
    "        dW_ih = dW_ih + np.dot(hidds_up_unnorm,ins[c].transpose())\n",
    "        #next hidden state for next iteration \n",
    "        dhidd_next = np.dot(W_hh.transpose(),hidds_up_unnorm)\n",
    "        \n",
    "    #reduce size of gradient \n",
    "    for params in [dW_ho, dW_hh, dW_ih, d_b_hid, d_b_out]:\n",
    "        for w in range(params.shape[0]):\n",
    "            for v in range(params.shape[1]):\n",
    "                if abs(params[w][v]) > clip_params:\n",
    "                    if params[w][v] < 0:\n",
    "                        params[w][v] = -clip_params\n",
    "                    else:\n",
    "                        params[w][v] = clip_params\n",
    "        \n",
    "    #last hidden state\n",
    "    last_hidd = hidds[len(inputs)-1]\n",
    "    \n",
    "    #return parameter updates, loss, and last hidden state \n",
    "    return dW_ih, dW_hh, dW_ho, d_b_hid, d_b_out, loss, last_hidd\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365f4afe",
   "metadata": {},
   "source": [
    "#### 4. Define function to check gradient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "691d12a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import uniform\n",
    "\n",
    "#inputs chars, target chars are lists of integers. hid_st is the initial hidden state \n",
    "def gradient_check(input_chars,target_chars,hid_st):\n",
    "    #global -> allow these variables to be modified from within this fxn\n",
    "    global W_ih, W_hh, W_ho, b_h, b_o\n",
    "    no_checks = 10\n",
    "    \n",
    "    noise = 1e-5\n",
    "    \n",
    "    dW_ih, dW_hh, dW_ho, d_b_hid, d_b_out, loss, new_hid_st=loss_iterate(input_chars,target_chars,hid_st,5)\n",
    "\n",
    "    \n",
    "    for param,param_up,prints in zip([W_ih, W_hh, W_ho, b_h, b_o], \n",
    "                                 [dW_ih, dW_hh, dW_ho, d_b_hid, d_b_out], \n",
    "                                 ['W_ih', 'W_hh', 'W_ho', 'b_h', 'b_o']):\n",
    "        \n",
    "        update_shape = param_up.shape\n",
    "        param_shape = param.shape\n",
    "        \n",
    "        #check that parameters and updates are the same\n",
    "        assert (update_shape == param_shape), \"ERROR: dimensions of parameters and updates do not match\"\n",
    "        print(prints)\n",
    "        \n",
    "        for i in range(no_checks):\n",
    "            #returns a random floating point number in range \n",
    "            rand_int = int(uniform(0,param.size))\n",
    "            #derives sample from flattened parameter array at random integer \n",
    "            rand_samp = param.flat[rand_int]\n",
    "            \n",
    "            #replace this value with itself + noise\n",
    "            param.flat[rand_int] = rand_samp + noise\n",
    "            \n",
    "            #calculate new loss after modifying this parameter \n",
    "            _,_,_,_,_,loss0,_ = loss_iterate(input_chars,target_chars,hid_st,5)\n",
    "            \n",
    "            #replace this value with itself - noise \n",
    "            param.flat[rand_int] = rand_samp - noise\n",
    "            \n",
    "            #recompute loss \n",
    "            _,_,_,_,_,loss1,_ = loss_iterate(input_chars,target_chars,hid_st,5)\n",
    "            \n",
    "            #return this value to normal\n",
    "            param.flat[rand_int] = rand_samp\n",
    "            \n",
    "            #print gradient (numerical (actual) and analytical (proposed update))\n",
    "            gradient_actu = (loss0-loss1) / (2*noise)\n",
    "            gradient_prop = param_up.flat[rand_int]\n",
    "            \n",
    "            #error of analytical gradient (add a small number to denominator to prevent division by zero)\n",
    "            relative_error_grad = abs(gradient_prop - gradient_actu) / (abs(gradient_actu + gradient_prop)+ np.spacing(1))\n",
    "            print(\"actual: %f, analytical: %f, error: %f\" %(gradient_actu,gradient_prop,relative_error_grad))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bec454d",
   "metadata": {},
   "source": [
    "## Part 2: Running AMSGrad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "95c5b283",
   "metadata": {},
   "outputs": [],
   "source": [
    "####HYPERPARAMATERS FOR AMSGRAD\n",
    "learn = .002 #learning rate \n",
    "beta1 = .9 #parameter for first moment\n",
    "beta2 = .999 #parameter for second moment \n",
    "epsilon=1e-8 #to prevent dividing by zero\n",
    "\n",
    "####Array to hold loss every 75 iterations \n",
    "Loss_AMSGrad = [0. for i in range(1500)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "a0670e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 1\n",
      "POETRY SAMPLE\n",
      "----\n",
      " eeeeeleeteteeeeeleleeeeeteeeeleeiteteieee\n",
      "tneeteeeeleeieeeeeeeetetenleeetlleetlti\n",
      "nleeeeoleeeneeleeeeoeeeeeete\n",
      "eeeeeee\n",
      "eileeeeeeeeeeeeeeene\n",
      "eeeee\n",
      "teneeeoeeeeeie loeeeeeeeeeeeeleeeelleleeetleeet elieteelteeeienelel illelteelenenlteeneeelelleetieeeeleeoeeneelieeeeetlieieeeleoeelreleeneeeeeieeeeteeneel \n",
      "----\n",
      "iteration: 75, loss: 135.464826\n",
      "POETRY SAMPLE\n",
      "----\n",
      " l s ii   lili   i l            lo le i li l   ie lllniiiini re  lli l ileco iiiii l  io i   ill  iil lioi  i   llmsnhl ilo   r lll   lil  i  ii lil hl i      lo    li  sle iill il  h tol o   iis li      li ir le l itilr ill lso rli ili iii lln     lii   olnriiitl  ali  o iiini fl    oio l  il iio ll \n",
      "----\n",
      "iteration: 150, loss: 152.159536\n",
      "POETRY SAMPLE\n",
      "----\n",
      " sttrraurrrnrinrn rgrlotrtrstsstlglgtrlrtregtlerttllr nrtldteninrttarrtllrrttrnetegrrrtrnttirrlrnrrrrrrhritrrentritnrreunsrrrenrrrteiltvreartrrsgttnlirrstrrritggrrrttsrtartrrrittrtrrrtnrlrrrrntrrtrgtsrrrrrnsgtareaur rtrrhtrrtrgesrrtslrttrtlettrtitnrr rlarnrtmrtnngrirroeurnrtleolrterogtetlrrsrnrrgtttr \n",
      "----\n",
      "iteration: 225, loss: 161.992099\n",
      "POETRY SAMPLE\n",
      "----\n",
      " eeeeh\n",
      "rtieiierriiidrrnrerrrirn\n",
      "ei\n",
      "dnetereiirteiretririe,eeiro vivtdtirrr nituirei\n",
      "irerniey iorerrditernn tirdaedend\n",
      "thteerdnrer reiinfiiteteiiiriidreyntyirmrmiriii eiriirim eeirhhiieit\n",
      "ns riertrhi irrrerendtineiinrryrirriier trindt\n",
      "reei\n",
      "ieiininiitetrrdnnirnr retieieirit\n",
      "ertneihltirtiigri\n",
      "vinirtrehye \n",
      "----\n",
      "iteration: 300, loss: 167.467767\n",
      "POETRY SAMPLE\n",
      "----\n",
      " dt tl  uettuehiAtoho t\n",
      "etetfo l\n",
      "retlfdeo ru  uriluese l   twiwtrltheowe \n",
      "rueuoete  u\n",
      "n etdt edo ra  teewhttr t htth  \n",
      " dtrw\n",
      "uelueeftto ldy\n",
      " h tmlt ltel u    ddlu feodfttotet fptwte dlete tfhtl,  lh  t,el\n",
      "etuutf seoheftloeleerh i deuettdboutdedleetstttdt l eteedptdiontellor httwd ot,ete\n",
      "nele,e\n",
      "e edtm \n",
      "----\n",
      "iteration: 375, loss: 173.241533\n",
      "POETRY SAMPLE\n",
      "----\n",
      " nnnaria nirinr naen\n",
      "yyiia annna \n",
      "llnninpi hiniillinn aaiipsaanangpianinnnpnihadnippe nnnlnn\n",
      "ieennhainnahninnilih scmlecinn,lna nnaniiaipnsnuaa\n",
      "npcpiihyp\n",
      "a i ninl\n",
      "npyin\n",
      "a\n",
      "n nyaya ann\n",
      "ip iikainniilnpninhncnhe\n",
      "ynienoiycnnna dsiaannnpr\n",
      ",nnnpahnpaai eiaiipiinpniiynnnnathan\n",
      "ianigipnlaeaaiyeapia pc\n",
      "eaianil \n",
      "----\n",
      "iteration: 450, loss: 178.278286\n",
      "POETRY SAMPLE\n",
      "----\n",
      " not r\n",
      "o insaaoitlit  tst td tww tn o trwn adnoadttle   tgaa anh sra irg detai it adnndcsitrtt ntantnnnito at wap appocant \n",
      " d  ann oa  ttoa trwdattgen neetl\n",
      "t ntrh  sirtw   eatnl tt a  tnpssno   r tstt er  tutlt a     ltoa aa tnnaot tar  so    t notapltt  r stntpdpp o n,t tn tnssa istoadn onp ttp tt \n",
      "----\n",
      "iteration: 525, loss: 180.384126\n",
      "POETRY SAMPLE\n",
      "----\n",
      " etieeheithee.thheege eteiaeteehtifdervt  eaaterietmihetsegttetehiittemietiebehibhthlthuhitaleeiaiteiuetitaeh tmheulietatidttcagetuutgtei eieheteieehtheuehetlghuteeelttahettt\n",
      "dttcvsegiegelegeeeeeiheiiisebiiii etigtlhhilele tteg,eiteheeudleoue\n",
      "iett eteeac ghttiitiatteitteiiieuith e ietlee tstiiigeieat \n",
      "----\n",
      "iteration: 600, loss: 181.291153\n",
      "POETRY SAMPLE\n",
      "----\n",
      "      woy uie of    i o  s    i l r  ci b   odl       i  iobdfn   s i   r iei  a edno dot brsy  s a s dfh sy  s bo\n",
      "o r eis teoys  e  b nn\n",
      "iin e shn iy  die oiyi oa o    l s sc om   dgb  g      h s  o b fra ie s ad  sy   eyys  ebye   u  odd     g ssea   ns   d     e\n",
      "  l   o     em   aei   f,i   s  d   \n",
      "----\n",
      "iteration: 675, loss: 184.444962\n",
      "POETRY SAMPLE\n",
      "----\n",
      " esso n ctise ehcseoeesstte tetrle teesletoeeeoee\n",
      "s   otsessheeo aeeeness ml ottle etetllesehes c s eesse\n",
      "ecreestes s sd e oesete ee estheth itseetl,edse eseeesdoeet seotii ie e seae tseeees ssetn eelhs oess s eseere eseee estlet sesdtse tnctees  stede t esse  soeoebtostsst hste ehme toeesoseoemeeh e \n",
      "----\n",
      "iteration: 750, loss: 189.784552\n",
      "POETRY SAMPLE\n",
      "----\n",
      "  gs heehhi hhs hssrhgdsgehs,\n",
      "oihghtshhsos  oghsl  g s hhh shgesghhhe ehgshstnhhrhhh\n",
      " h ehhi hiehdohe hrihs svdnh ohehes reserhthaesvhivhg h g hih ees whhhhve snh eo h hhehsghgerhvshhr\n",
      "hg hss tehhethghhivh seoshhdesngs h thehsen eehvdseahhheih dghheshssa shh  hssshseh esshgh\n",
      "hish hh  hoh shs shs evhh \n",
      "----\n",
      "iteration: 825, loss: 191.683543\n",
      "POETRY SAMPLE\n",
      "----\n",
      " n ln a.eradun ekl n  an s    n ada al asbn dfuun lb da  .y reasvnee\n",
      "da lee t.osna ne  r\n",
      "nia nu pw eneal kananraen o t.e na s asaan\n",
      "ttiwtn da k a lsmalt  d s  ,nthi  \n",
      "a um rtaa b lk\n",
      "nn a uaatlaanf eoanta a ,nbrgt tmumauk enrl uw r\n",
      " a  k\n",
      "\n",
      "un.naabtabaa nn i dn Aa u sa\n",
      "lnaab mas oafa wgn aa.ututg eeaan  \n",
      "----\n",
      "iteration: 900, loss: 190.781552\n",
      "POETRY SAMPLE\n",
      "----\n",
      "  rriurm,a e m rtfe,gema,e ipt aof autwfna dae an eiu etot pr utapa ate pan eeghrraan  t ea,i etgnm an tmt umatantaad,uadar otsad\n",
      "m,i,,aismamateaua ii e tnat oe,   iaepue,dtoaaot a  teit ugui  mi raaumudoa   ntaautue dudaneo mgtt o ittruu ntmao t  argmttn etat up\n",
      "seen taee si     adrmctatahutm t,gaes \n",
      "----\n",
      "iteration: 975, loss: 192.056934\n",
      "POETRY SAMPLE\n",
      "----\n",
      " g a,tbsi d an  t utt svsahnstsd  ns  nt nsaosat  \n",
      "stiosta ttupt sbet ,tsa ddc,,ugs nagsd  t  dhsa sthddssttisatuosn ao bdoosthd aht sdi a s tdshatretd on tors  rhytstats stu ssiltstttsnst uvdus  o tn esntstha asd \n",
      "tbsdgan  ss  n,ht n t ttt  d  snu  nswtdtua  hdsogshdas tdldnstsasss  dohudstat sa t   \n",
      "----\n",
      "iteration: 1050, loss: 191.488530\n",
      "POETRY SAMPLE\n",
      "----\n",
      " A  e s  tei  c r r l  ccl    n e nrccho tcsc eceocey o hyi i  okcton  Ahrc ol rew  hcc c vcc  yv y hu irror o,cnw  c     rch,sce   rceee s t c rAsnecci cec rc ei   gylAc l occ ceoclcA  \n",
      " cnc Ar i   nunwiche   ceclt i he t   i swc, eeA  :craa  cc  cyccnl   c    cc rg  e A  lec v,   u mcc cwnei ee age \n",
      "----\n",
      "iteration: 1125, loss: 194.324651\n",
      "POETRY SAMPLE\n",
      "----\n",
      " TTtgnoaeytutthtIahr o  ott yoIpIdhiror youg  coy tr g orthotty  rngcr ,ye tsarttheo hh rt,th ht  ur rhiyrtiIcthcroa iuIy cdtrcooIyh rdbdrtohcr  htrrhIuhtioiea roretrodythr  t I  th   ry\n",
      "do hsout  hh h o  b    o ur Ih t rrlrtn trhen rho  l aorryh on  hoyt direyrhtrrytho oIrrti  a \n",
      "cuhIdotAh.otur oncg \n",
      "----\n",
      "iteration: 1200, loss: 196.685180\n",
      "POETRY SAMPLE\n",
      "----\n",
      " od\n",
      "\n",
      "tak wo u\n",
      "awhluHHwtdteHete a,aruattdae ededew,wttd He t \n",
      "oewtta HtthadroHhurehaaueaa heua\n",
      "t ooaeott nrbdtt H\n",
      "ir\n",
      "lddthdteHHttaeoaer tlautdhuta llaeha  eteueddeweeawalrrdttaftut aledwtatbtod,Halwtawatltka ll aeaodtwraaueuo\n",
      "wmaateadhduohcdebeutuaawha Heae \n",
      "eaaa\n",
      "rttntutktadcdt\n",
      "ootatttoolae oad \n",
      "----\n",
      "iteration: 1275, loss: 195.506344\n",
      "POETRY SAMPLE\n",
      "----\n",
      " yeyyeleaoe oooyeo Ioooeeosyreseetmroeeytyeetesde\n",
      "eeete  ooeeyoooesolyeeyelweyoyom oolyeyreo,oyeye esyme\n",
      "y eyoe rdedw\n",
      "r em ly w dydoeoeses dy ohe I eaoelfyl fye tor,t me osoeyy  ea  oyoytm edyey weo\n",
      "lyyytoeesyya\n",
      "lyos\n",
      "lrsymyorreoydmeoyeelo eew\n",
      "aeyty yym\n",
      " er  eye  e  o yyde demyeym hy \n",
      "----\n",
      "iteration: 1350, loss: 195.812905\n",
      "POETRY SAMPLE\n",
      "----\n",
      " aty  alle m aesmnl,rkae ,am ke a\n",
      "a  hleaiam    a e da\n",
      "alasttmamaeea\n",
      " a asn i,leT aaainaisaaf,e acl al hna  \n",
      " c,a   ttw a y aae,Tii\n",
      " t  f  s\n",
      "aldaeaaeaey aeafh\n",
      "am,fiefffaAlaat aaa e\n",
      "o\n",
      "  \n",
      "----\n",
      "iteration: 1425, loss: 194.279635\n",
      "POETRY SAMPLE\n",
      "----\n",
      "tr h   reerr,  vsr ,r er \n",
      "sps se t  r r  s rns rr\n",
      "rr  rs  rreen asemf s \n",
      " ssnly,ohe e phr s w  ra o hftra Hoese   rsnr s  s,o rr\n",
      "  nre okglan rnnTee l\n",
      " s ghsppnet\n",
      "n s f no rr \n",
      " r t \n",
      "----\n",
      "iteration: 1500, loss: 189.313366\n",
      "POETRY SAMPLE\n",
      "----\n",
      "tdhesloorernogoo oso toli loo sTuuoa\n",
      "leolluoootfou\n",
      "oypoueoo  Toot\n",
      "olmoo lladlnllouoolyoelhothsyooll odt\n",
      " ioot o,\n",
      "loololoy\n",
      "hot\n",
      "roroollrtsu\n",
      "laosoossoossw\n",
      "e\n",
      "\n",
      "trftor lrvkrowodTla logdooru\n",
      "sootcudddoy\n",
      "olm ol sdiolo\n",
      "iol\n",
      "d deoh uolda \n",
      "----\n",
      "iteration: 1575, loss: 187.187336\n",
      "POETRY SAMPLE\n",
      "----\n",
      " hgla lnnoydoo llnog nl oo nooadnorh rl erog a  nueo ho o  ol o  nl\n",
      " o   ortu   wtny nooo gl  oo\n",
      "g  oe osned oheeyt.goon  ygn ooo  ln no  d hgoleysn sygd l g. hsao\n",
      "n  roo    o g o nt s  nytdo eoonno\n",
      " yao  \n",
      "----\n",
      "iteration: 1650, loss: 183.565120\n",
      "POETRY SAMPLE\n",
      "----\n",
      " dmttsu ermaletheetossao sl pddcootsdltthsetnsehsw aelaanaeay ie\n",
      "sasn da aleltassra ,atptyhvlodrortydtatt e\n",
      "eti alsaataepotntpyrt ttoess hsuosehalnh fsoa td leorslroatdtsttattt dseshwahsa\n",
      "l saeror tsta \n",
      "Ahed e te ul\n",
      " pppp   a ntteec\n",
      "a tptrdantdlsaeaaaAo ppr aulha harstedteeeeegaeelty\n",
      "rtelAssdsr \n",
      "----\n",
      "iteration: 1725, loss: 179.486579\n",
      "POETRY SAMPLE\n",
      "----\n",
      " p elule  yhefiylebet y e,y, yo \n",
      "yeb y   e\n",
      " \n",
      "  eryy yl\n",
      " ye be ubb,yyyo  yyrytely th,v\n",
      "eue y erletyi f\n",
      "e t,elyep eee u nyfyw o  e, heesyy y  rbb t l urb e  y   b\n",
      "floelefu\n",
      "y yy ,y,Tb\n",
      " h r re,  ooeyyr,b,loeeT \n",
      "----\n",
      "iteration: 1800, loss: 175.261740\n",
      "POETRY SAMPLE\n",
      "----\n",
      " tird\n",
      "owrnswiae rcrisd \n",
      "nd\n",
      "a n\n",
      "gdohte nr\n",
      "ones \n",
      "hrsoorwrgcsoi rodhseoei wnwgsw\n",
      "ohwdraie hal or  oh\n",
      "eioraa\n",
      "oors w o odnelnr\n",
      "ceir a\n",
      "gnncewownco\n",
      "tooo i\n",
      "----\n",
      "iteration: 1875, loss: 172.194329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POETRY SAMPLE\n",
      "----\n",
      "hhlvlehhreollelehhleoepl ph ehrehg,rtteeoeolrrrreee nlrrrgrhleoprerrslrrhvmhtleoepshoiep i,ohgs  l l roooor,re\n",
      "----\n",
      "iteration: 1950, loss: 168.981894\n",
      "POETRY SAMPLE\n",
      "----\n",
      " \n",
      "agyordotgragigoooehdg\n",
      "ssloniratodngeggrlsdh nowg ghagoge rglsg onouaagh o  tatgnsgo gedaov\n",
      "adtd;n uenlesahenygdolldoaeotn\n",
      "nsgsgg oasnognnglgoaidr  ltuaoe g od tiloaaetnuggdgtehllot lgltoettsueubdwdtoogl\n",
      "----\n",
      "iteration: 2025, loss: 166.411392\n",
      "POETRY SAMPLE\n",
      "----\n",
      "h.a heoc vrhh cecnWvct\n",
      " ee y WvtycvvW c vcccWc W eWdWeh  ft Achcutcy, eycW\n",
      "et v edh taenldec Wuo\n",
      "heepccvhleeh vces ldc   ,yWeWiW  eaWev,d A  s,hvcoc \n",
      "t\n",
      " cb,c ceWh  ce ne  hd cccdcbv cc h c\n",
      "----\n",
      "iteration: 2100, loss: 163.104501\n",
      "POETRY SAMPLE\n",
      "----\n",
      "ymln iatiupitotiwdaw,pto espletaf\n",
      "im lp sl, si aaiafnii cti,si  t rseoa i dt  ti,nptpi loofi,hiiioilrfteidniidd\n",
      " siil lip tua sf\n",
      "  o i,upt tpn\n",
      "ith ht io f,eyk tipoae\n",
      "----\n",
      "iteration: 2175, loss: 160.468997\n",
      "POETRY SAMPLE\n",
      "----\n",
      " u in uhueahTt brh dstnyaedtnhi tiTs\n",
      "eiaiashuet,ntmeae tty vo\n",
      "sashe smthvvhsa\n",
      "tt \n",
      "hhiaei mOceshns ush uhhnheeh ametrhatm hmt uhaethanttmhdtmrastata\n",
      "ht hnntafnvmH\n",
      "iel thhtt evatshanhnush \n",
      "\n",
      "tn\n",
      "at\n",
      "thh\n",
      "emauhtedehhmn Teuuh\n",
      "fmnauau t,d \n",
      "----\n",
      "iteration: 2250, loss: 159.112223\n",
      "POETRY SAMPLE\n",
      "----\n",
      " h nvse l eers rar \n",
      " rsen h  lypr r rre o  r  w hdlhlereeertttee tsyr t nnha\n",
      "oe sS rei or \n",
      "  fnrl Sanen nrft  af  nS\n",
      "r  e otrhegstrltssntto hstr n   e lr, .r  t e ahnl lrtee nSe  n a so r  uuh\n",
      "rs S    s noTrae  hpp nn ndtee\n",
      "ttSearlel  etane \n",
      "----\n",
      "iteration: 2325, loss: 160.322716\n",
      "POETRY SAMPLE\n",
      "----\n",
      " lni\n",
      "eg  ie eee,wg eo  etshe g egeererb  i\n",
      " eaht er  g  eerth'srre  oe\n",
      "wr'reer tb eat   e.gHh ueer'rote\n",
      "  ,art'm srerr gr 'ieTe\n",
      " gtehte'e neegrroeht npe wte nre  e  eeh o'roreh eeg'eo e   ergegegr gr\n",
      "aeegne m  asgh  teerryeeee'h e  er ete'' rgee nere ''  thee '   '''''e   Hrel  tr \n",
      "----\n",
      "iteration: 2400, loss: 160.666618\n",
      "POETRY SAMPLE\n",
      "----\n",
      " th eo bsstih,  brh r iiu  \n",
      "eebsuto e o,ubtuuih of ii boa\n",
      " aes sb aruta  bh d b  e  ire iuifes u ti  ,tuo uboebe\n",
      "uee ss ac vreh esilcb\n",
      " ichthiu dausgi t yue ub gu \n",
      "b o  a itoabrst rruubb y i i r \n",
      "----\n",
      "iteration: 2475, loss: 159.532071\n",
      "POETRY SAMPLE\n",
      "----\n",
      "tttohgohthy gtu yrlntgSho,ioid   h  n' hh, hthn hh otetghsn hh,  oo thn,o Shnharhoa ,rhgn\n",
      "ni y aelhiht  omtoiohghtooo wthth ehd\n",
      "eadt hlnhsuhonw    nhh  ohta r   \n",
      "----\n",
      "iteration: 2550, loss: 159.895171\n",
      "POETRY SAMPLE\n",
      "----\n",
      ",,  ltbeneen,leiiaesdee usba,lbe ec,teiios  sblok   ecet kr   elai r e e elkdeec kk tesre,\n",
      "rbiiae;aen \n",
      "----\n",
      "iteration: 2625, loss: 155.931239\n",
      "POETRY SAMPLE\n",
      "----\n",
      "etrfrhedrwd fehdeedwh hcIharrrseoehehhreaewryrdrFrrsdedfteerhh\n",
      "enrhrhT hrtdottee\n",
      "eerhfyyrse \n",
      "haa heedrhdtdfhfe hhhdt\n",
      "dd\n",
      "hreewhhdrradeh arsafrhwhonTe\n",
      "hhatsrthei rfehndr \n",
      "----\n",
      "iteration: 2700, loss: 152.962595\n",
      "POETRY SAMPLE\n",
      "----\n",
      "tamiuhrTainuiiddiTenleiunn\n",
      "ndoramungdr,ac roseimnrted\n",
      "n\n",
      "enr\n",
      "rdr g nrsrcttrhcore er,sra\n",
      "cgrter eiise\n",
      "nr\n",
      "sniccde\n",
      "schncdcTeg:T\n",
      "iuchuuu\n",
      "nrltucd mgr\n",
      "cTdheu\n",
      "crtihe nrnsn eaTsycct;pcpeafechgcenrcru ehc\n",
      "gauctsiercre\n",
      "rcorr \n",
      "----\n",
      "iteration: 2775, loss: 150.239693\n",
      "POETRY SAMPLE\n",
      "----\n",
      " smas' so isA moooeo a,orumru msrmsi sio egum susss o o moio\n",
      "s'ggmm' smmgro o isoo oim tigos'd i oo sgs ,   siggism mi mmioimm h \n",
      "mim ommmmemm'mios i sgsmuomtim  au  ,sotohbiemsm s\n",
      "  vio  ,tso'mt eeomsruvsi  itouyoi   mtm sysr o roooggof oo mmnutt mmoo m mm m oe g  a uuoosrs\n",
      "'   i eomhsos 'ig\n",
      "m o \n",
      "----\n",
      "iteration: 2850, loss: 148.263693\n",
      "POETRY SAMPLE\n",
      "----\n",
      "  ntei hah setop,t nvppp eatyft eomenta tst  smp\n",
      "a ee mee ,Se tpp gaa,ae her pispu ,p  mopttnyuptcna n n tanpphpitaae eeapdnt\n",
      "ytea  ntade yptvn \n",
      "s ed t a\n",
      "ctpeemn  pmatm papnua phmy t\n",
      " a mp mof nSnhenn ina  e oneenpane\n",
      "n temiy,tepeyg apaes n  na psntleopptaotiooha ntdpn\n",
      "----\n",
      "iteration: 2925, loss: 148.473301\n",
      "POETRY SAMPLE\n",
      "----\n",
      "   \n",
      " we heca  e   s  \n",
      " ae     ee       kne hr  .      \n",
      "e        o  m i gii, h  it\n",
      "  k ,gb      \n",
      " c,m k   e,\n",
      " m,  , b  e,\n",
      "\n",
      "      e   a kntetge  e  kvh          \n",
      "  , i   i. : \n",
      "  \n",
      "    \n",
      "  l  c k, ca  \n",
      "n\n",
      "l\n",
      "    ee ken    e f    a     ,fec\n",
      "k le  , \n",
      "----\n",
      "iteration: 3000, loss: 150.124797\n",
      "POETRY SAMPLE\n",
      "----\n",
      "    e, i    eeh   ie  sh\n",
      " \n",
      "  e ee  eerohy hhe  oy  yh   \n",
      " r    e   h hhh   ee  \n",
      " . m  tS   e e e h   :  \n",
      "y    e   i h     e ngh   sat  vh a haeeh    ds:  n aee ee a :  n   ay   v       vsv  mnem wm   m h\n",
      "----\n",
      "iteration: 3075, loss: 148.347756\n",
      "POETRY SAMPLE\n",
      "----\n",
      " eru MLM\n",
      "M  MeOyMMOemM ir MeMMMMM MMMMM  MMro\n",
      "L M MMoeeMMMM\n",
      "MMM lMMs\n",
      "emyMeMM\n",
      "MmMahbeM dMtMrMM mMOMfMv l  MMM LM  fMLMMM LmeM\n",
      "nMM  pMmMOuMMLvMMee  mMMsMMMm M MMM\n",
      "MMhM M tMe MeMMMOM MMMMwMMMiMeMMM Mc MMgMMaeMMMMuuLe MM eM M MFl  MO\n",
      "M MMMMcerM\n",
      " r eMMLOMM MvMM\n",
      "MeMe nMMMM.MrMeMM\n",
      "aM  \n",
      "----\n",
      "iteration: 3150, loss: 146.774281\n",
      "POETRY SAMPLE\n",
      "----\n",
      "i deddfoA, Ansdydw dgssdo\n",
      "nsdsso ou nydyfsuleastdtbo Mnftansf,dd\n",
      "ddnotewotft A fi dAAtHydidnpeA medt  wAbi,oA\n",
      "s fldas sofsdd ustlftbteddd  :: \n",
      "aad  MidfofclyblAoddorasi :y:oylet\n",
      "rlIAsiMa ito de\n",
      "eoA d iobi.AyondngddAdtrhsde t \n",
      "----\n",
      "iteration: 3225, loss: 145.748146\n",
      "POETRY SAMPLE\n",
      "----\n",
      "  p o eoeela\n",
      "thu lscocopoeleyah. sIee seeeahaeeeeeoap shf,iehaseoooeettyletsseeh\n",
      " lheo osuesllioteset eoeleiolhoso c shie aesoeldeee\n",
      "oapoesaiaoeol e\n",
      "oeoe.tllalotepleeceesl t Tieeofedolshopmc e saaEe yte lop  p l we,eauets\n",
      "----\n",
      "iteration: 3300, loss: 143.381956\n",
      "POETRY SAMPLE\n",
      "----\n",
      " EhsCd. hCe Ee eC\n",
      "ECCEuEeCCC\n",
      "CfieEa.CCdCeECCECCeaCCtCeCCaCECteiICe CCCCreriIIEdCr eEECE  EeEeE eEeCCCC.deeCse\n",
      "ECCCCaC FCEdCCEyCCerCC:CdesEE.aIEICmeoEreECC\n",
      "CtCasECCoEdCC\n",
      "\n",
      "eAiEEoeEdeC CewEEiCCCCiIoCCe\n",
      "Ce e..CiEECEoiCCmtrCCt.e\n",
      "CEet\n",
      "EeeCi \n",
      "----\n",
      "iteration: 3375, loss: 141.633079\n",
      "POETRY SAMPLE\n",
      "----\n",
      "eeheoeyhinee o.eofeeth nesrreerasteterefsaeorthyn etdrdte hr\n",
      "o eat rhtyoreetfeeattniaso t   t\n",
      " neeteeeoeoe dtl eeo\n",
      " \n",
      "oreh taaek:aftoeepeetieAueaeenoiiTn dfs nrt\n",
      "eristedfloe o :rsr eohtosarnho\n",
      "o: aTeydt rteeerrdbt \n",
      "----\n",
      "iteration: 3450, loss: 139.392564\n",
      "POETRY SAMPLE\n",
      "----\n",
      "wo:oeasddadowse ss ewwwoadsdsld ww dawwawd wl w ,wdadeolo ssasa,swtln :esetsdbwtswwdadwdroecddawwteoawo;o adts\n",
      "osao \n",
      "odooeadtsenasdwwowt\n",
      "wdystoea owwlwn edwbwded\n",
      "waaaw  ed w, sdad adssduase,wadl wboewcaawdewasaa swtb idwda \n",
      "d \n",
      "----\n",
      "iteration: 3525, loss: 136.696194\n",
      "POETRY SAMPLE\n",
      "----\n",
      " ehfktetnlct  ef ldfedlawteeeaeatgefa eobta eaneh lateceeaetsyeeaaesfhelftow aetaetutsed ,nady,AaAetouab aAeaeCctltvesefettesaee caeflyeeaedstl de tfkaayteototweotetnadtwaeo,teet teep,ttmteeetfbenke ebaatta snstaiaahyssf eotetbadne rtrtteteftehtse teadeseaoeaid ney hefdpf tweaslasor e tayoe  baaectad \n",
      "----\n",
      "iteration: 3600, loss: 135.470521\n",
      "POETRY SAMPLE\n",
      "----\n",
      " r.\n",
      "scoyiage\n",
      "io\n",
      "Ayy. acai\n",
      "ems  risni\n",
      "ie\n",
      " yllyheA\n",
      "i\n",
      "\n",
      "ooraiih\n",
      ":isiees\n",
      "nib iyi \n",
      "onlnsnsyr\n",
      "i yeigi\n",
      "\n",
      "iriss.\n",
      "nyssr y i\n",
      ".sc\n",
      "risa \n",
      "risioiciiue\n",
      "rihh\n",
      "iodiad sm\n",
      "iia\n",
      "mli i i\n",
      "ysn\n",
      "i seii .dt\n",
      "\n",
      "y,is\n",
      " syhl\n",
      "\n",
      "gAa   \n",
      "\n",
      ",\n",
      "isees\n",
      " oet\n",
      "i\n",
      "d nyo\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "rsdyss \n",
      "----\n",
      "iteration: 3675, loss: 133.464164\n",
      "POETRY SAMPLE\n",
      "----\n",
      "roni esohe tsehhvyleboeps nbes  b  h  ayii nihecii bb ioo\n",
      " tibaikket  iiialr   ns  bn\n",
      " e s iooetl e  rn hhee iseeeileeei   e bi tdiiubl,o a\n",
      " i el oa ieslci o  lpisov a:iyneeeyis\n",
      "voll\n",
      "ti goofr eievnhirai o igliig rl\n",
      "lin  ii  oei n a\n",
      "  eoe b pfi geW iahWa, i i\n",
      " lt \n",
      "e \n",
      "----\n",
      "iteration: 3750, loss: 133.046102\n",
      "POETRY SAMPLE\n",
      "----\n",
      " ohee  oechkam weIohwooe,oo\n",
      "go orerewo \n",
      "hooehmomeohnetooohe gt eehoecohhbe ai oeIlohheeehheh ehgorrotoros e oehwgwob gogrhhr\n",
      "mokhochoher oroewo rhomodew\n",
      "heonrweh\n",
      "emmoheocrm oo oohoonho o\n",
      "----\n",
      "iteration: 3825, loss: 131.230384\n",
      "POETRY SAMPLE\n",
      "----\n",
      "hbt ooeagirossoseeihhsostroeeheshhegssssheroaoggtbgysheel hbohasthwthhs ttiwgesshweionohaewrhehohotsroeibthateeftosnwteesgewseArbhseli eihawahtosssoaehhoeoeoabeyeeoehrehniogehm heeheleasogorrocwosrsohesesataohh,oh \n",
      "----\n",
      "iteration: 3900, loss: 128.895054\n",
      "POETRY SAMPLE\n",
      "----\n",
      " aaa;ysoaydpafsfeaf; fr\n",
      "ordptrTTIaogodduyfdrfff\n",
      "adiooff lafaafypf hf;n,r.TrtTddf d trfdcp didTfSdyr\n",
      "fp;dfrfadeyagaghdafdrefgehagfofrdpahnTsof;grdyaaaa;dTyrofdddgd hfy TddyfT,T;staddf  gypueygrfotgfrsoo\n",
      "fohkdfraddd; gfarfTpyrdatshyfddfTfuhddddpmlahtr;h \n",
      "----\n",
      "iteration: 3975, loss: 128.551471\n",
      "POETRY SAMPLE\n",
      "----\n",
      " nod  dro , deeeednanb\n",
      "nisseefgnand nt\n",
      "tei te l odndoed oeb,\n",
      ",dgse,aeeed tdrao\n",
      "r todyen,n e nd bdhdsd ede,ese  d o,Tnlnen.ee ,dyse emhdel  g g o eeee dpTen\n",
      "d\n",
      "mnhs noi,aTt,deoisydtenedseneosTn  td gt nn mdndrmnen egd  en hdn tehdTg\n",
      "\n",
      "ddde t eed   \n",
      "----\n",
      "iteration: 4050, loss: 127.234104\n",
      "POETRY SAMPLE\n",
      "----\n",
      " i n \n",
      " een s    e  h f bo s et\n",
      "ee     e   i   ,s  Ths     t ,eub \n",
      "sh lsee  esbb f     is s s  ons\n",
      " e  e  steTt set e    \n",
      "ss   e t o es  n  \n",
      "hfA  e     f    is n  i ye  fi\n",
      "   teii ss eseee\n",
      "  ny  s e  if     u   \n",
      "----\n",
      "iteration: 4125, loss: 125.995875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POETRY SAMPLE\n",
      "----\n",
      "  G   i\n",
      " sasssea  h n s emeaiesnheev es et\n",
      "ait ma  osnene a, amm m v e snstvAdars ehpatsss,e  se  s da  m n n  sneeeet tre  ss  dis a eve  ee\n",
      "----\n",
      "iteration: 4200, loss: 126.321582\n",
      "POETRY SAMPLE\n",
      "----\n",
      " crliIhbeis t heh,hii\n",
      "  nhehtdnngiscoipoT oh\n",
      "ooh a,  rns rhgttitis, eatytiis ietotiiihi htihdtphp,hh h  haetw nmihs gigdtteeetiihektiehn\n",
      "tentahhhgiegeghdoghhiuytih enh it stogsethii ihnnegntmtngh ,bthhehteihtk\n",
      "----\n",
      "iteration: 4275, loss: 123.862664\n",
      "POETRY SAMPLE\n",
      "----\n",
      " luus t  o hy tt      \n",
      "     t  t a dt   u   hg  ht  rh   r   nh n  k   ns\n",
      " \n",
      "e\n",
      "      ul    me,o  b                  s    u  u \n",
      "    l        tesTe   \n",
      " e  Ai      t     d h h    \n",
      " t  h  ub nt t  M   t h h   \n",
      "h t     iM t  t o  n   c   m  t    o   w    o   tuu \n",
      "----\n",
      "iteration: 4350, loss: 122.779076\n",
      "POETRY SAMPLE\n",
      "----\n",
      "hiteheoe hliu?ttd?eredhOh?\n",
      "erb, rOei?m?hrtltlmi? ehOr?rr?sehise u heerlwgOd??rlu\n",
      " e?i?ev\n",
      " dl?i  urh ho hh\n",
      "eh ??ewgh  lerhesehtaO?  ?\n",
      "imeoo  o\n",
      "esuue?utmbOe??hh dmtlmHi?u?lsdlirueedokOehrrhuederd?t?hh\n",
      "rdhe svhuwereO?d tolhtrluh?urteeIsidehes \n",
      "----\n",
      "iteration: 4425, loss: 121.555278\n",
      "POETRY SAMPLE\n",
      "----\n",
      " urrr\n",
      "c u, nru'chea,rcdr\n",
      "e'ddchocv iruosdwwcw'fec,w sachin dithuau ds du ccrh cwhechir,hr lkslreoa ceogcec'wednuttbc,nnacss i,nr phbhn rutp r c rr h rek  er  u'r sns rhihnc  eaeec e \n",
      "edect.u  l   hnrcc  renc n,ehicueeesescahdehs c \n",
      "chac h hiica ,hrhhcaueddhcoererclmecih\n",
      "e \n",
      "----\n",
      "iteration: 4500, loss: 119.318077\n",
      "POETRY SAMPLE\n",
      "----\n",
      " loetvhoipel.toovotoe.mWvt\n",
      "tatotvetptnevWWnfoateoroWoeedoesogin\n",
      "irrt tvW\n",
      "vonoeaoovtlnvto\n",
      "ni e\n",
      "netntoiiea nn.rrnaba nievn,nt  e orvttnndnWesin.hcnntn f n eioetnoohon\n",
      "niWnisnW lv\n",
      "foo \n",
      "topuWnei \n",
      "ttoocrhnoovnnno H t,n\n",
      "oWl.,.viaarttWvn ra. \n",
      "----\n",
      "iteration: 4575, loss: 117.588622\n",
      "POETRY SAMPLE\n",
      "----\n",
      "ws   e  so s,   O  ottow n     t\n",
      " rk,   w  oec olue  ww os wu  a ro\n",
      "     up\n",
      "fos n ko  ow   \n",
      "io      \n",
      "w, ,k ,u t,  yw u e w  O w    Ow ww\n",
      "  ws    w   n ow  mw o o       s l l\n",
      "----\n",
      "iteration: 4650, loss: 116.420162\n",
      "POETRY SAMPLE\n",
      "----\n",
      " t\n",
      "se ;   ev  aphfr\n",
      "sa  rvhv essgfsrve,te\n",
      "suaa ert,aii,  s s rsH;r si rea a  eahg shrthv hr a   s,i f i svp\n",
      "as ia evrsgehrafhhr ira hs  herv    feet voce rs \n",
      "elpvatr ssissneled e et ,raeees f  v isrt y;nh  a a errv,g f  r asssrse gas eaa ,e  rhes rg rae asavitr v veetasr     rr r \n",
      "----\n",
      "iteration: 4725, loss: 114.177682\n",
      "POETRY SAMPLE\n",
      "----\n",
      " pir mhphttregeeeeutmeeehreeerawmheeetyhgewmemhrelhaeeie.genedmpegodecertihteeree goleteiaottttiami aie hieagiyeeohpeureeenhehuoattwgpe etthaulyeetuetwraeeuheehheugt oNeh\n",
      "uuu chemteerttagame, httctchyteeeeaieett ee etemeoe  m\n",
      "huewmteg  t tg.,e e  eemeottwts Ighw; \n",
      "----\n",
      "iteration: 4800, loss: 113.321890\n",
      "POETRY SAMPLE\n",
      "----\n",
      " i sh t bntdr, aaia  t tAadbe da  i ston   lleivcalleiatr o wa i tle eti  ia aoaml  t tm n\n",
      "t  rbdav,eyil  aol btwt  , w ya sat  aantoy a s l  ale,r  sctm  pe rltobi nhn  pdni \n",
      "     dobltttotn,,   hmalt  ltaaa, iht l  ttbdnb arotgbr.iatny ,l \n",
      "  hitaop  yi \n",
      "----\n",
      "iteration: 4875, loss: 112.293740\n",
      "POETRY SAMPLE\n",
      "----\n",
      " uhtu euthnndsnit\n",
      "ltd,stulooudl,\n",
      "a ,d,seues t elueduTddifa  u\n",
      "lttot thitadtebuuuuw\n",
      "a;tu t,ualuuhtuie tup t lu man\n",
      " uoubasn d sool,ldhsttouitoutut\n",
      "ul shus  htuda tsudoauow uto euotdu tuytsi uu\n",
      "nitnt\n",
      "autd twduteul sow\n",
      "aosuu auneu ituWs\n",
      "I\n",
      "rirttu  \n",
      "----\n",
      "iteration: 4950, loss: 112.112761\n",
      "POETRY SAMPLE\n",
      "----\n",
      " e  feirn net y eekrwe owstaaeei,t a w e  a\n",
      "taita wvaIa rra enew ahgf \n",
      " bebr \n",
      "kaaho wuen aayhkoae f chea,kkre eae \n",
      "h  eo   o, ,   io geahof o\n",
      "h nkrae ee w ralee\n",
      " to a r e k kehkhoe ibn da Iitve  fdeoesw ,aduhhn h  ka e wo h a he\n",
      "hh aahnnke hfet vhi n s arieh \n",
      "----\n",
      "iteration: 5025, loss: 114.575363\n",
      "POETRY SAMPLE\n",
      "----\n",
      "m  tma e  imoeoo  \n",
      "t   \n",
      "ed et ae\n",
      " aa otabot okmoeoits tnam  ai ntthn\n",
      "\n",
      " eeaa m.Aeactonyosoit e\n",
      " to  eea d\n",
      " s mheeaaaol tt\n",
      "----\n",
      "iteration: 5100, loss: 114.604398\n",
      "POETRY SAMPLE\n",
      "----\n",
      "ubuy   i r; ulsfgcuo ra \n",
      "clc curdi\n",
      "i sc t\n",
      "dr t iha hau  syriunl sa\n",
      "ol\n",
      "oa na;\n",
      ";b  hrit\n",
      " tudba t   lagh  u\n",
      "\n",
      "uaa;hyafl acrudehuswc yhr , ts   , lb rahTdar   aidl ytlor\n",
      "nisia iln\n",
      "p siil us rsr ilns\n",
      " u u ey;n\n",
      "----\n",
      "iteration: 5175, loss: 113.047625\n",
      "POETRY SAMPLE\n",
      "----\n",
      " otrioiynndoohy rre , horse  oooer'ehii noeyeiier,r\n",
      "arStvTghih'o'oooiiforgeyio,\n",
      ";rcoaoh,ie hA\n",
      "n i,hliv'nn'etr\n",
      "hhreedgiragoir,iteoeTi,iv ftdonethr'erevp oloergavirirt  \n",
      "----\n",
      "iteration: 5250, loss: 111.546560\n",
      "POETRY SAMPLE\n",
      "----\n",
      "d m nn uefn re i ahoy nfofnudrn d lmelsd\n",
      "ot n cnun;u  ocmAefa yn, eIho n nnc u efcnnemui  stpn ;i co nci i nheclntnioorn ofd neruomfBef yocfitng setcch m lrchuy nnoIgo k eo\n",
      "w  edcnoydh fu,gnle  io oui nm c \n",
      "----\n",
      "iteration: 5325, loss: 111.414663\n",
      "POETRY SAMPLE\n",
      "----\n",
      " tthgTs aon ehwsglThbtgetyt,\n",
      ",eTroehnstreef\n",
      "abshssb,ithts\n",
      "hetsvnh\n",
      "tn\n",
      "tTsgyt se\n",
      "taha baaseol\n",
      "tnneerob,thuT\n",
      "TLsse gr uhh,ettat\n",
      "toreri,thmht,esemghuhhe\n",
      "oh o bso h\n",
      "yhhis,tdhsoeT vTbtwt eheawht,t\n",
      "----\n",
      "iteration: 5400, loss: 110.070690\n",
      "POETRY SAMPLE\n",
      "----\n",
      "teuogthn lnttntn o'iesttsssnsmnnvon nthoosenyv u uhutntoootouunntnchainfusfoe,'uoavs\n",
      "sufytnns'asr'tnotty'yeWnopttsl tlsof't\n",
      "ftl gmsnsotyi\n",
      "ot,s nnyyrfnosoolotb yheneennaansntonstyo\n",
      "t ol'nvycovss noagynwst\n",
      "----\n",
      "iteration: 5475, loss: 109.258540\n",
      "POETRY SAMPLE\n",
      "----\n",
      " nl,WW  peo, es\n",
      "dr, wno t w shwh  ortls,lWn WslwwloelWgww  e,wolWl,lyledd lewlWow\n",
      " nsoelltrolewWhoTl lel \n",
      "l hw lw  w neo\n",
      "eooo\n",
      "eWl d\n",
      "odoWtdol,Wcrrol\n",
      "botl,wwwwrrwr\n",
      "ws relr,vn el otht wnodwll\n",
      "geotvel  so rw tos ohw,oo wlWledownWo  \n",
      "----\n",
      "iteration: 5550, loss: 108.710985\n",
      "POETRY SAMPLE\n",
      "----\n",
      " sdeitgsiesenhusteswe.hatstr'sonytea   es eesete gfere egh see seotntettenskt oirtrett  tset t hss ,\n",
      "htseet t et ektsa mdte tdrtesoo\n",
      " tse s e uep eaooee,oys s Iwe teedots snb to o steeik gesy fae dnheseatk oteso etsts  sbo\n",
      "tesp roeBed gdo st et ssdsee b s n  tdn eed   n eeneu etLt d ctsee   ten \n",
      "----\n",
      "iteration: 5625, loss: 108.789154\n",
      "POETRY SAMPLE\n",
      "----\n",
      " \n",
      " o   ob    ua  ha nta n resaeptF\n",
      " sodat t aahsf ornea ia , \n",
      "maoppo ten ed\n",
      " o   ttl eoa ipto   o pn  ah  aterir, fht w\n",
      " eg \n",
      " frt ntaloro opn yttntw   aahhtnpep l toeoe o se asmw toasoestne\n",
      "tt   oetdi  ri r\n",
      "----\n",
      "iteration: 5700, loss: 108.814547\n",
      "POETRY SAMPLE\n",
      "----\n",
      "rssgnen .nssnuos  unar geig s  ugiley.snsn  csnsbnasl  sulln\n",
      "gsesglnlsSs   rsnntur  ynns  gSy aoi gss  ennelgs    snn ucsrsnnn smugey nbuagnsm lcinnnn us nss u u neoc\n",
      "b uuns enm t  d rBstysnsn\n",
      " mn  si se sg lsn nnn ssacnn \n",
      "----\n",
      "iteration: 5775, loss: 110.707806\n",
      "POETRY SAMPLE\n",
      "----\n",
      " rc l\n",
      "teutoewDvA eaDoelhDh Dercschal \n",
      "e ce,llDaheDlodwDeigiDsl\n",
      "D ws t iDc\n",
      " helD,Deewyhrll le eoDe\n",
      "l uaeweetsev.lD\n",
      "acih al mlee Dcl\n",
      "DeteAwD,woeee l\n",
      "Sci lelthlcDw,lyreseDerlee eDehhtleeeal aede,ee,Dlwwcat\n",
      "lscl ea Diighf    haynDuelme c nlseinlhreAsa lfcoterawD\n",
      "----\n",
      "iteration: 5850, loss: 109.809310\n",
      "POETRY SAMPLE\n",
      "----\n",
      " wi otck p tm saoe  aw\n",
      "lTtl cc e lphttshrtnto ttrs   th Sr ymut eith\n",
      "ttt o    hpat td ttvSaf ESffprtTw uh  svu  elthihhaeiw    thc eupcrp tc pcsflc noe   sur psieutyk  tt hgtpnoearetps haehranr tefr glhr    nf,c, re  hrstney ,ppccchhcEttf\n",
      "hWi hhrl oom  apiia  fyu oussT mi \n",
      "S \n",
      "eh \n",
      "----\n",
      "iteration: 5925, loss: 109.618202\n",
      "POETRY SAMPLE\n",
      "----\n",
      "w arr giaa nidgrigih na, snne\n",
      "aate n ne\n",
      "\n",
      "d inol ge  ei nitentddge\n",
      "n er gdnne ei\n",
      "eiad,eginhng wehnA ngAAinhigi,  geaa\n",
      "n\n",
      "ehna\n",
      "a nd ,l ia g  ig,erhr gnieneaithailid wne.teiagd ewrnihnwtgn\n",
      "----\n",
      "iteration: 6000, loss: 109.015533\n",
      "POETRY SAMPLE\n",
      "----\n",
      "i areiorwltte o  wtti dttei n lhee \n",
      "oriwkroer wh:rhiwwr\n",
      "t  i,eeehAtu\n",
      "ot\n",
      "\n",
      "i etw :ttr    wwt r ea ongrtdi\n",
      "oeerptpt\n",
      "dee  :ih:mtnrtwte  o\n",
      "ttin\n",
      "hpa\n",
      "eetrwhgedp tdntitetwdh ntiwo.: sAM,e\n",
      "ootth\n",
      "iwtpwet po htdtewAetdoept or\n",
      "drh   tii w\n",
      "----\n",
      "iteration: 6075, loss: 108.373276\n",
      "POETRY SAMPLE\n",
      "----\n",
      " ir l\n",
      ",tiiew\n",
      " lreiiiheew\n",
      " w,lserlhli\n",
      "eFteimi \n",
      "hgi e h teWih l eert i isehltwee  I e eo lloerFielww \n",
      ", ,iliee titlvttIn,\n",
      " ih   o e oeelr m r\n",
      "hetene iioih ieere telti i\n",
      "e t whi tiie eeriv o  \n",
      "eli  o   t tewistt\n",
      "----\n",
      "iteration: 6150, loss: 113.936760\n",
      "POETRY SAMPLE\n",
      "----\n",
      " whtrthh\n",
      "ytnew rno d ee t  me yceeyh  dtth eoggshhhhyr wt obdhdhh \n",
      "deehrd\n",
      "erdhtt t drt eh e  sohwtd hdhht tv dtthhdev ey'ythlvenhh en eAe  e n et  y\n",
      " ehcyc t th \n",
      " tltte    wyhh.yh whe hrntr mb  eyt tnm h \n",
      "----\n",
      "iteration: 6225, loss: 113.079253\n",
      "POETRY SAMPLE\n",
      "----\n",
      "aimhoirnsetvelnevoaivnaa;eienbr i naaeeaeeehaeunniv;toheeIyv eiuaaintn\n",
      "aeeineInavI e np\n",
      "nveia;nhirena;n nunIeorg iiI;eviv  tsgoagimHiirrvsgiuoiairivniutadkwfo\n",
      "aondrgrsvaea\n",
      "rsv\n",
      "inin\n",
      "ogiae\n",
      "suIroalahe vagayoaHee ,IsIii ewurh\n",
      "rti \n",
      "----\n",
      "iteration: 6300, loss: 112.526863\n",
      "POETRY SAMPLE\n",
      "----\n",
      " r\n",
      "ga uoeldSr  er\n",
      "Hra ,eT  ta retrteula\n",
      "tola   \n",
      "S e   e,ihn ntcort ueSStanuirdo oeueu w ene wa  el deu a ued  i aewSiuyence eoo\n",
      " w r eewehSe,r \n",
      "e entT,artmnaSpv, nSSe t  ,b\n",
      "n,,w anren;nS S op\n",
      "ewSnnadd \n",
      "----\n",
      "iteration: 6375, loss: 111.791333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POETRY SAMPLE\n",
      "----\n",
      " ky yynyyyiv elbty ayy nnay yt  thony,woyl i h a\n",
      ",ooye,yyHyl kye lak nloy k,yly y l ii l iiky,s  ,iby h ykykiyylsy l ei ywill   l  al i  ki l,el Tt i iyaihh li n  hyy y  eky  yye i oiait\n",
      "i n    klikla ivhkiy l y taeeoi i yitiatk akykkyyy\n",
      "i y  lidolbileelhii ni  okti ykilynf lyk y   \n",
      "----\n",
      "iteration: 6450, loss: 110.489135\n",
      "POETRY SAMPLE\n",
      "----\n",
      " ou,ulosoo, b\n",
      "f unnwsgnumegigivuginioLvun, ioov\n",
      "hi,is,oi, f ,ueidru.uotvoH\n",
      "n,.uoul.nigroooou.,uho.,gg\n",
      "uiD\n",
      "spnu,nonO oTilo.uf  iufoolni, noln nn\n",
      " ni\n",
      " y  fIi\n",
      "f b\n",
      "au,tu oiuuioaouunsoufouupvi,ieoiionutiu ;uLooiedouwogwnn \n",
      "n,i\n",
      "kuDun,gumou,ou \n",
      "----\n",
      "iteration: 6525, loss: 109.784659\n",
      "POETRY SAMPLE\n",
      "----\n",
      " eeip tir e,iegey  h lin t eig,nn enn,raTfe g llggu,oi c \n",
      "cidtp sinioones  y fe nieg myno  , ,nhyyt\n",
      "a    gnee,eg,rgiari ca w g \n",
      "ri eap vw  y ns  so,ih n ,e or  lf  ofstie en,ppsrs ,b gson: orrne   rnoo,a   giyainen eggaea i,io\n",
      "tis,sti n eg,unekrpv, ,e es \n",
      "aure ig rirokan  a h,  s,,dii giihncg nre \n",
      "----\n",
      "iteration: 6600, loss: 108.571075\n",
      "POETRY SAMPLE\n",
      "----\n",
      "  yawwta rrfolrooooo wooro w \n",
      "oloste afata y haofoeoOoh  oowavoylt oto rae \n",
      "ol oefaepsy eohi efi,ioam  cvaw ofeiworvotso o nor r eesofstat,eBo   eoe o,oo  ouodohatrordr \n",
      "----\n",
      "iteration: 6675, loss: 107.310377\n",
      "POETRY SAMPLE\n",
      "----\n",
      "cvkon nr ,ua d ua wa,o   l uaa aroou      vhe, , asl,lnc  H v  ho,a g  ls    ,r o, hv l\n",
      "uatv ggr  i rv o gddy adu  atfr  alo   a re t o  ooaa, al ra\n",
      " ba ano,  a  v sIuo,Tov oa rl g  igar  ee   s v t \n",
      "----\n",
      "iteration: 6750, loss: 106.278727\n",
      "POETRY SAMPLE\n",
      "----\n",
      "sde eeyemeesrnoocew \n",
      "noeeds edeseTs,\n",
      "eeedu secee\n",
      "oberereessWrseeueoereese ee\n",
      "ee,eeeeee csoesetcldreole reece heeeeemog\n",
      "eetT mceTs eocdsa\n",
      "ee ssTs eerrry To\n",
      "eeTd;,cs\n",
      "lsio emdoo eeedwAeee\n",
      "myioeeeege seT,e e e resie  edee m Tr\n",
      ",dmsn eeee\n",
      "med\n",
      "mt \n",
      "----\n",
      "iteration: 6825, loss: 105.535260\n",
      "POETRY SAMPLE\n",
      "----\n",
      " A ,enme ,ree e   eA ek eW  rreH\n",
      "  ed,keee    e  e cs tc e   e eye    es t e  e aef\n",
      "e e  B  te ekaerae  e  edleu oel e neeee s  ee ee i iose,stveea ee e ntet\n",
      " nc\n",
      "ea:esec e eegsee  e eeedaote e , ee eHenhehl e om eisn c\n",
      "  rl  e  e t oese nre eeeeO \n",
      "----\n",
      "iteration: 6900, loss: 105.327194\n",
      "POETRY SAMPLE\n",
      "----\n",
      "h?e s\n",
      "erorotfh.erhaomieAde,ee ehtemo,h?ie\n",
      "?eehrforhmhf?e eomeseooheo\n",
      "ohoe\n",
      "rheereierhrreteoeorh ?rh e,v e,nfh\n",
      "e\n",
      "vheeo om?o\n",
      "rn?rtviee eo\n",
      "ofeouhhhO\n",
      "oee  t,eh h ?ehhoeheim,cmwf\n",
      "?h,et.obryefhme \n",
      " \n",
      "----\n",
      "iteration: 6975, loss: 104.249660\n",
      "POETRY SAMPLE\n",
      "----\n",
      "Ae ttol w  dylkuts  wsh Asssos tArtldo  h dl u Alm ol n l   nls\n",
      " h.lllTltt nlleslleoont shtldooe tts\n",
      "Asflte htb lttt   a sss Io lell tt\n",
      ",,etot   o hels.dess A.lt o osaa lsoAld l Ansst sttogn  lsso olthanfs   alotrtlsl   a lltllsotltteAAg  tlllloyt oelo, .hhholhutl \n",
      "----\n",
      "iteration: 7050, loss: 103.155996\n",
      "POETRY SAMPLE\n",
      "----\n",
      "rerc,sdanh yrh tssIhmha de efs s swls,sBsD,as duhdhsud Onars,hBtgarl,aeshs  lh asefe,ra ee arsg mneals, a e,aleaha ekdulrer  a aher d,e aesnendndl eseshsassrd\n",
      "----\n",
      "iteration: 7125, loss: 102.567369\n",
      "POETRY SAMPLE\n",
      "----\n",
      "rrW ssa \n",
      "usarnlroth,eiYeil d YtdeoY\n",
      "sorteln .oi.t   h h\n",
      "nwkoam tthlY aalitrt Yetors tsre Yrv.i YiYYer raY\n",
      "rst  net\n",
      "hYt   tY\n",
      " .tYi nt lblaa.e  l seotta Y toptaisYanrteYYa t itshda.eiY.nraatBmsa.byyl\n",
      "ta, en  eYhpo.o td \n",
      "----\n",
      "iteration: 7200, loss: 101.447981\n",
      "POETRY SAMPLE\n",
      "----\n",
      " tTtggigggTdh e\n",
      "ee\n",
      "trgrptbTsrut:\n",
      " ototsrhmegsfear\n",
      " dturtotet\n",
      "ir TTsgileeioyrtrdpggg\n",
      "o\n",
      "gTbTt\n",
      "hrwreTgTrthsrst\n",
      "ig\n",
      " enhgr \n",
      "ereTrtrT aTsyge\n",
      "ahhpBcttrygwt\n",
      "ghrvTorgltrh\n",
      "\n",
      "r. aho\n",
      "gTtwrh\n",
      "srrrgTrT\n",
      "\n",
      " rhgetr\n",
      "p ehit rhh \n",
      "----\n",
      "iteration: 7275, loss: 100.588825\n",
      "POETRY SAMPLE\n",
      "----\n",
      " dnnehr nmr,arhdncawdf\n",
      "ddlddrelnaaeg rasho.wnoaarldldao lfd ffaabtlln nmlt aramdlanahdiawTfdTn ioTadrarirn\n",
      "ndnhtthl ofrd\n",
      "r,arr:rdatnddloaddd\n",
      "aalet,nntrgarndrrwdear ia\n",
      "rrddddwrrroe d a\n",
      "\n",
      "sat r,\n",
      "\n",
      "ndyoihigvrnairdria\n",
      "adoheatl ard,r ef d derhraah \n",
      "----\n",
      "iteration: 7350, loss: 100.150900\n",
      "POETRY SAMPLE\n",
      "----\n",
      "mo idwe efnt o iiiet on ithesu  iS   liuoteohonl  ddd  Sn ynoelsnnnon\n",
      "eenh   noihidhye d o bd ndidfo,teiueboennsyoaoe ho  Al d i uyel ebwbfono\n",
      "nrB y onnn  on  nneosnnri shn inbi oSa  enn id n o nnouw osii hk i  ohn whh onuoh nuo n ow \n",
      "----\n",
      "iteration: 7425, loss: 99.492969\n",
      "POETRY SAMPLE\n",
      "----\n",
      " eeifetahise raiisrL\n",
      "ts eicsfnm  tvinmtseseaeruas er if hiceett eufsndo. e etsnef .el s etsie.e cifaetes tesestte aoftsshiiceiytefihesitea\n",
      "meneeiiss titumu  \n",
      "----\n",
      "iteration: 7500, loss: 99.091034\n",
      "POETRY SAMPLE\n",
      "----\n",
      " \n",
      ".avhhveonT\n",
      ".v npnn\n",
      "v\n",
      "oTnusentntearhu thheeeh\n",
      "Thne\n",
      " e te aeedyuamvht\n",
      "eef l:hamhetat  eae u?hyaytt\n",
      "yn\n",
      "vlehytucthv mehoa  tthkae\n",
      "thtyh\n",
      "yevhtvv\n",
      "yhthr ,o\n",
      "h\n",
      "\n",
      "thvhdt\n",
      "eysnhvhttIv\n",
      "nhoa\n",
      "tmonh\n",
      "eaut\n",
      "t\n",
      "s\n",
      "ewhhey,uetuauante\n",
      "veTm \n",
      "----\n",
      "iteration: 7575, loss: 99.725771\n",
      "POETRY SAMPLE\n",
      "----\n",
      "  o oviettee eu .v ev eloea e  ir     v treO e sh  vaf  endcr  aehtv\n",
      "e  hcvvco to   tee ve e ttv m rev e ete o i    ei et  vv etteeeat h    uv oulcaeecv hhese\n",
      "  a t tso   srf uo bpn  eo ,v ,t h o  ne e e  ih on  \n",
      " \n",
      "----\n",
      "iteration: 7650, loss: 100.060941\n",
      "POETRY SAMPLE\n",
      "----\n",
      "srosrrharearmei h\n",
      " wr b aee eesspshsW\n",
      "tresresos;rseid\n",
      "ar eres,swses cceWnseom c\n",
      "wms nrsem rvr,rnmsrdspwer ammcc nsuhe\n",
      "sehese\n",
      "Wmsedrrserrrscccrdnerreemperrehenr rrrWW\n",
      "v eeWe ,uyrne \n",
      "----\n",
      "iteration: 7725, loss: 99.596824\n",
      "POETRY SAMPLE\n",
      "----\n",
      "fnnco konh et\n",
      "mtoe noieanfit  eoosrio mnh\n",
      " vneoifhiheethoo.o\n",
      " wi  ttyoosse:dtsihio ion\n",
      "rmnIn rifoitcpanntty etiiAsiwocncm  eioneeonfci nSipitecatceiienenn  aynepimnmosmln  sfotompntoccc\n",
      "ocn wn tia \n",
      "----\n",
      "iteration: 7800, loss: 99.247293\n",
      "POETRY SAMPLE\n",
      "----\n",
      "roccgyaooiburtrgvaogngm onegfnsohccyynofoaloaml gn goedgvon\n",
      "Innfnc cy  ocacyggyl ggh or\n",
      "porygobgoghoog oghtnsoogtgatvhyeg grrrgga paa\n",
      "Wrrefooooeroos eauorngrgec egr\n",
      "----\n",
      "iteration: 7875, loss: 99.955598\n",
      "POETRY SAMPLE\n",
      "----\n",
      "co aner,aoaaaet rahthlcrchl haaarthTm oaaac ah a haarar no au hhahatdhakphahhtubraa aradarmcutlo  uohoara o o rrmn nre crnm\n",
      " ar hra n au rhvam;eba d fnco od hlh,rduacugauho hn,aeaa aouccmahtlu mhdhdh nomeh rAim gr   ahh nLrma rr maha rc \n",
      "----\n",
      "iteration: 7950, loss: 99.432600\n",
      "POETRY SAMPLE\n",
      "----\n",
      " tseslrino \n",
      "feso\n",
      "\n",
      "eaisiseisolsepiwoi eetessiie eeig \n",
      "hests\n",
      " rreiiis\n",
      "idi\n",
      "lesniwnaspsiihi;einei\n",
      "rsel stsresalyiesite;aintnlen eishisseti\n",
      "rrsoiieiisetsi a \n",
      "swcnisrisrissrnwaeiii \n",
      "giiispwecsgow;isngessatti;r\n",
      "seiiisfhl stieiti\n",
      "wtiiiestess\n",
      "sisfwwrll\n",
      "oiia\n",
      "ss\n",
      "is \n",
      "----\n",
      "iteration: 8025, loss: 100.902209\n",
      "POETRY SAMPLE\n",
      "----\n",
      "ts s   h den dah  ;  ntr   e    u h nh  oib   ;us    hoe sth;ce  hhd w ee  sah i h     \n",
      "e  w  o  \n",
      "         f    ne  wi s hhe\n",
      " hb     hihe   i ptwm\n",
      " o     i e\n",
      " e h ooiv  en \n",
      " t i t o t\n",
      "ow b \n",
      "   i \n",
      "----\n",
      "iteration: 8100, loss: 100.241687\n",
      "POETRY SAMPLE\n",
      "----\n",
      " ftgirt d.,oipshrhag eerite, ie eieohowhtMt tthr.lmihpgtt,iow eigeo;c ,j t,ehhilort lah th diilessuse, e ri ef,ide\n",
      "ah i th,o\n",
      "hriih,  ,hhenegeso ,tce,tlgtwhhhhihrihrogleghheepi\n",
      "eptuytshtede, eohhduone ;,tlheghhththinh h,manhiiitlaht rrnaneii hsoe,letlhifkrnauHshstte ,teahnlhtinrno\n",
      "c tr ttineg uh;e \n",
      "----\n",
      "iteration: 8175, loss: 99.281109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cassandra .DESKTOP-UDR64AF\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:87: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "C:\\Users\\CASSAN~1.DES\\AppData\\Local\\Temp/ipykernel_5248/1968604197.py:34: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = loss + -np.log(probs[c][targets[c],0])\n",
      "C:\\Users\\CASSAN~1.DES\\AppData\\Local\\Temp/ipykernel_5248/1968604197.py:32: RuntimeWarning: overflow encountered in exp\n",
      "  probs[c] = np.exp(outs[c]) / np.sum(np.exp(outs[c]))\n",
      "C:\\Users\\CASSAN~1.DES\\AppData\\Local\\Temp/ipykernel_5248/1968604197.py:32: RuntimeWarning: invalid value encountered in true_divide\n",
      "  probs[c] = np.exp(outs[c]) / np.sum(np.exp(outs[c]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POETRY SAMPLE\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "probabilities contain NaN",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\CASSAN~1.DES\\AppData\\Local\\Temp/ipykernel_5248/1189690056.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[0mout_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_unnorm\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_unnorm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#normalized log probabilities\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[1;31m#selects one character randomly, however taking into account probabilities of each character\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m             \u001b[0mchar_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msz_char\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout_norm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m             \u001b[1;31m#one-hot vector representing selected character (for next iteration through for loop)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[0mchar_vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msz_char\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: probabilities contain NaN"
     ]
    }
   ],
   "source": [
    "#loss at iteration 0 should be determined by size of vocabulary and the sequence size (number of times to unroll RNN)\n",
    "smooth_loss = -np.log(1/sz_char) * no_steps_unroll \n",
    "\n",
    "#AMSGrad again uses the mean (first moment) and varaince (second moment) of the gradients from previous time step; each parameter gets its own mean and var\n",
    "Grad_means = [0 for i in range(5)]\n",
    "Grad_vars = [0 for i in range(5)] \n",
    "Grad_vars_corr = [0 for i in range(5)] \n",
    "#Grad_vars_corr = np.zeros_like(W_ih)\n",
    "\n",
    "\n",
    "it = 1 #iteration number \n",
    "st = 0 #point at which to derive sample from data (data pointer)\n",
    "count_ind = 0 #counter for storing loss every 75 iterations\n",
    "\n",
    "#maximum number of iterations \n",
    "iter_ceiling = 200000000\n",
    "\n",
    "\n",
    "converged = False\n",
    "#while not converged:\n",
    "\n",
    "while it <= 112500: \n",
    "    \n",
    "    #reset memory of RNN if we are at iteration 0 OR if we have reached end of data strand \n",
    "    if it == 1:\n",
    "        print(\"ITERATION 1\")\n",
    "        #initial hidden state to use after restarting \n",
    "        hidden_st = np.zeros((hidden_neurons,1))\n",
    "        #start back @ beginning of data\n",
    "        st = 0\n",
    "    elif no_steps_unroll + st + 1 >= sz_dat:\n",
    "        #initial hidden state to use after restarting \n",
    "        hidden_st = np.zeros((hidden_neurons,1))\n",
    "        #start back @ beginning of data \n",
    "        st =0                         \n",
    "    \n",
    "    #slicing data to match sequence size/number of unrolls \n",
    "    input_slice_char = all_poems[st:(st + no_steps_unroll)]\n",
    "    target_slice_char = all_poems[(st+1):(st+1+no_steps_unroll)]\n",
    "    \n",
    "    input_slice_int = np.zeros(len(input_slice_char),dtype=int)\n",
    "    target_slice_int = np.zeros(len(target_slice_char),dtype=int)\n",
    "\n",
    "    \n",
    "    #define these slices in terms of where characters appear in dictionary \n",
    "    for c in range(len(input_slice_char)):\n",
    "        input_slice_int[c] = characters.index(input_slice_char[c])\n",
    "        target_slice_int[c] = characters.index(target_slice_char[c])\n",
    "    \n",
    "    #check gradient estimates (eventually remove break)\n",
    "    #gradient_check(input_slice_int, target_slice_int, hidden_st)\n",
    "    #break\n",
    "    \n",
    "    #print out samples of model progress every 75 iterations \n",
    "    if it % 75 == 0:\n",
    "        print(\"POETRY SAMPLE\")\n",
    "        char_vec = np.zeros((sz_char,1))\n",
    "        #represents seed (start with current character)\n",
    "        char_vec[input_slice_int[0]] = 1\n",
    "        char_indices = []\n",
    "        h = np.copy(hidden_st)\n",
    "        for ch in range(300):\n",
    "            #forward pass through hidden layer\n",
    "            h = np.tanh(np.dot(W_ih,char_vec)+np.dot(W_hh,h)+ b_h) \n",
    "            out_unnorm = np.dot(W_ho,h) + b_o #unnormalized log probabilities \n",
    "            out_norm = np.exp(out_unnorm) / np.sum(np.exp(out_unnorm)) #normalized log probabilities \n",
    "            #selects one character randomly, however taking into account probabilities of each character \n",
    "            char_index = np.random.choice(range(sz_char),p=out_norm.ravel())\n",
    "            #one-hot vector representing selected character (for next iteration through for loop) \n",
    "            char_vec = np.zeros((sz_char,1))\n",
    "            char_vec[char_index] = 1\n",
    "            char_indices.append(char_index)\n",
    "        \n",
    "        #convert indices back to characters \n",
    "        sample = []\n",
    "        for c in range(len(char_indices)):\n",
    "            sample.append(characters[char_indices[c]])\n",
    "        #append characters into continuous string \n",
    "        sample_txt = ''.join(sample)\n",
    "        print('----\\n %s \\n----' % (sample_txt,))\n",
    "        \n",
    "        \n",
    "    \n",
    "    #calculate gradients from no_steps_unroll characters \n",
    "    dW_ih, dW_hh, dW_ho, d_b_hid, d_b_out, loss, hidden_st=loss_iterate(input_slice_int,target_slice_int,hidden_st,5)\n",
    "    \n",
    "    #smoothing loss so loss is averaged and not plotted/printed as erratic across iterations\n",
    "    smooth_loss = (smooth_loss * .999) + (loss * .001)\n",
    "    \n",
    "    #print loss every 75 iterations \n",
    "    if it % 75 == 0:\n",
    "        print(\"iteration: %d, loss: %f\" %(it,smooth_loss))\n",
    "        Loss_AMSGrad[count_ind] = smooth_loss\n",
    "        count_ind += 1\n",
    "    \n",
    "    \n",
    "    \n",
    "    #copying weights before update to check for convergence \n",
    "    W_ih_prev = copy.deepcopy(W_ih)\n",
    "    W_hh_prev = copy.deepcopy(W_hh)\n",
    "    W_ho_prev = copy.deepcopy(W_ho)\n",
    "    \n",
    "    #updating weights \n",
    "    for param, param_update, index in zip([W_ih, W_hh, W_ho, b_h, b_o],\n",
    "                                                    [dW_ih,dW_hh,dW_ho,d_b_hid,d_b_out],\n",
    "                                                   [0,1,2,3,4]):\n",
    "        \n",
    "\n",
    "        #first-order exponential decay (momentum beta 1)\n",
    "        Grad_means[index] = (beta1**(it+1))*(copy.copy(Grad_means[index])) + (1-(beta1**(it+1)))*param_update\n",
    "\n",
    "        #second-order exponential decta (root mean sq beta 2)\n",
    "        Grad_vars[index] = beta2*(copy.copy(Grad_vars[index])) + (1-beta2)*(param_update ** 2)\n",
    "        \n",
    "        #correcting bias \n",
    "        Grad_vars_corr[index] = np.maximum(copy.copy(Grad_vars_corr[index]),copy.copy(Grad_vars[index]))\n",
    "        \n",
    "        #updating parameters \n",
    "        param -= step*(copy.copy(Grad_means[index])/(np.sqrt(Grad_vars_corr[index])+epsilon))\n",
    "\n",
    "\n",
    "    if np.array_equal(W_ih_prev,W_ih) and np.array_equal(W_hh_prev,W_hh) and np.array_equal(W_ho_prev,W_ho) and it>0:\n",
    "        converged = True \n",
    "        #break\n",
    "    else: \n",
    "        #increment iteration count by 1 and datapointer by size of unroll.\n",
    "        it = it + 1\n",
    "        st = st + no_steps_unroll\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fae89a",
   "metadata": {},
   "source": [
    "## Part 3: Running ADAM (Adaptive Movement Estimation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76175ad7",
   "metadata": {},
   "source": [
    "\"Adam is a replacement optimization algorithm for SGD for training deep learning models. Adam combines the best properties of the AdaGrad and RMSProp algorithms to provide an optimization algorithm that can handle sparse gradients on noisy problems.\" (https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/#:~:text=Adam%20is%20a%20replacement%20optimization,sparse%20gradients%20on%20noisy%20problems.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c998c05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "####PARAMETERS FOR ADAM \n",
    "\n",
    "step=0.01 #step size  \n",
    "beta1=0.9 #first order exponential decay  \n",
    "beta2=0.999 #second order exponential decay \n",
    "epsilon=1e-8 #to prevent dividing by zero\n",
    "\n",
    "####Array to hold loss every 75 iterations \n",
    "Loss_ADAM = [0. for i in range(1500)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7aaf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#re-initialize weight matrices and bias terms\n",
    "W_ih = np.random.standard_normal(size=(hidden_neurons,sz_char))*.01 #connecting input to hidden layer\n",
    "W_hh =  np.random.standard_normal(size=(hidden_neurons,hidden_neurons))*.01 #connecting hidden to hidden layers\n",
    "W_ho = np.random.standard_normal(size=(sz_char,hidden_neurons))*.01 #connecting hidden to output layer\n",
    "b_h = np.zeros((hidden_neurons,1))\n",
    "b_o = np.zeros((sz_char,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0a0e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss at iteration 0 should be determined by size of vocabulary and the sequence size (number of times to unroll RNN)\n",
    "smooth_loss = -np.log(1/sz_char) * no_steps_unroll \n",
    "\n",
    "#ADAM uses the mean (first moment) and varaince (second moment) of the gradients from previous time step; each parameter gets its own mean and var\n",
    "Grad_means = [0 for i in range(5)]\n",
    "Grad_vars = [0 for i in range(5)]\n",
    "\n",
    "it = 1 #iteration number \n",
    "st = 0 #point at which to derive sample from data (data pointer)\n",
    "count_ind = 0 #counter for storing loss every 75th iteration\n",
    "\n",
    "#maximum number of iterations \n",
    "iter_ceiling = 200000000\n",
    "\n",
    "\n",
    "converged = False\n",
    "\n",
    "#while not converged:\n",
    "while it <= 112500:    \n",
    "    \n",
    "    #reset memory of RNN if we are at iteration 0 OR if we have reached end of data strand \n",
    "    if it == 1:\n",
    "        print(\"ITERATION 1\")\n",
    "        #initial hidden state to use after restarting \n",
    "        hidden_st = np.zeros((hidden_neurons,1))\n",
    "        #start back @ beginning of data\n",
    "        st = 0\n",
    "    elif no_steps_unroll + st + 1 >= sz_dat:\n",
    "        #initial hidden state to use after restarting \n",
    "        hidden_st = np.zeros((hidden_neurons,1))\n",
    "        #start back @ beginning of data \n",
    "        st =0                         \n",
    "    \n",
    "    #slicing data to match sequence size/number of unrolls \n",
    "    input_slice_char = all_poems[st:(st + no_steps_unroll)]\n",
    "    target_slice_char = all_poems[(st+1):(st+1+no_steps_unroll)]\n",
    "    \n",
    "    input_slice_int = np.zeros(len(input_slice_char),dtype=int)\n",
    "    target_slice_int = np.zeros(len(target_slice_char),dtype=int)\n",
    "\n",
    "    \n",
    "    #define these slices in terms of where characters appear in dictionary \n",
    "    for c in range(len(input_slice_char)):\n",
    "        input_slice_int[c] = characters.index(input_slice_char[c])\n",
    "        target_slice_int[c] = characters.index(target_slice_char[c])\n",
    "    \n",
    "    #check gradient estimates (eventually remove break)\n",
    "    #gradient_check(input_slice_int, target_slice_int, hidden_st)\n",
    "    #break\n",
    "    \n",
    "    #print out samples of model progress every 75 iterations \n",
    "    if it % 75 == 0:\n",
    "        print(\"POETRY SAMPLE\")\n",
    "        char_vec = np.zeros((sz_char,1))\n",
    "        #represents seed (start with current character)\n",
    "        char_vec[input_slice_int[0]] = 1\n",
    "        char_indices = []\n",
    "        h = np.copy(hidden_st)\n",
    "        for ch in range(300):\n",
    "            #forward pass through hidden layer\n",
    "            h = np.tanh(np.dot(W_ih,char_vec)+np.dot(W_hh,h)+ b_h) \n",
    "            out_unnorm = np.dot(W_ho,h) + b_o #unnormalized log probabilities \n",
    "            out_norm = np.exp(out_unnorm) / np.sum(np.exp(out_unnorm)) #normalized log probabilities \n",
    "            #selects one character randomly, however taking into account probabilities of each character \n",
    "            char_index = np.random.choice(range(sz_char),p=out_norm.ravel())\n",
    "            #one-hot vector representing selected character (for next iteration through for loop) \n",
    "            char_vec = np.zeros((sz_char,1))\n",
    "            char_vec[char_index] = 1\n",
    "            char_indices.append(char_index)\n",
    "        \n",
    "        #convert indices back to characters \n",
    "        sample = []\n",
    "        for c in range(len(char_indices)):\n",
    "            sample.append(characters[char_indices[c]])\n",
    "        #append characters into continuous string \n",
    "        sample_txt = ''.join(sample)\n",
    "        print('----\\n %s \\n----' % (sample_txt,))\n",
    "        \n",
    "        \n",
    "    \n",
    "    #calculate gradients from no_steps_unroll characters \n",
    "    dW_ih, dW_hh, dW_ho, d_b_hid, d_b_out, loss, hidden_st=loss_iterate(input_slice_int,target_slice_int,hidden_st,5)\n",
    "    \n",
    "    #smoothing loss so loss is averaged and not plotted/printed as erratic across iterations\n",
    "    smooth_loss = (smooth_loss * .999) + (loss * .001)\n",
    "    \n",
    "    #print loss every 75 iterations \n",
    "    if it % 75 == 0:\n",
    "        print(\"iteration: %d, loss: %f\" %(it,smooth_loss))\n",
    "        Loss_ADAM[count_ind] = smooth_loss\n",
    "        count_ind += 1\n",
    "    \n",
    "    \n",
    "    #copying weights before update to check for convergence \n",
    "    W_ih_prev = copy.deepcopy(W_ih)\n",
    "    W_hh_prev = copy.deepcopy(W_hh)\n",
    "    W_ho_prev = copy.deepcopy(W_ho)\n",
    "    \n",
    "    #updating weights \n",
    "    for param, param_update, index in zip([W_ih, W_hh, W_ho, b_h, b_o],\n",
    "                                                    [dW_ih,dW_hh,dW_ho,d_b_hid,d_b_out],\n",
    "                                                   [0,1,2,3,4]):\n",
    "\n",
    "        #first-order exponential decay (momentum beta 1)\n",
    "        Grad_means[index] = beta1*(copy.copy(Grad_means[index])) + (1-beta1)*param_update\n",
    "\n",
    "        #second-order exponential decta (root mean sq beta 2)\n",
    "        Grad_vars[index] = beta2*(copy.copy(Grad_vars[index])) + (1-beta2)*(param_update ** 2)\n",
    "        \n",
    "        #correcting bias \n",
    "        mean_grad_corr = Grad_means[index]/(1-beta1**it)\n",
    "        var_grad_corr = Grad_vars[index]/(1-beta2**it)\n",
    "        \n",
    "        #updating parameters \n",
    "        param -= step*(mean_grad_corr/(np.sqrt(var_grad_corr)+epsilon))\n",
    "\n",
    "\n",
    "    if np.array_equal(W_ih_prev,W_ih) and np.array_equal(W_hh_prev,W_hh) and np.array_equal(W_ho_prev,W_ho) and it>0:\n",
    "        converged = True \n",
    "        #break\n",
    "    else: \n",
    "        #increment iteration count by 1 and datapointer by size of unroll.\n",
    "        it = it + 1\n",
    "        st = st + no_steps_unroll\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec7e3a3",
   "metadata": {},
   "source": [
    "## Part 4: Running AdaGrad \n",
    "AdaGrad is a variant of Stochastic Gradient Descent (SGD) that automates the selection of the step_size parameter, which normally requires hand-tuning. It is dynamic because it uses large step sizes (faster learning) for parameters that relate to features infrequently encountered. Conversely, it uses smaller step sizes (slow learning) for parameters that relate to frequently encountered features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7c5805",
   "metadata": {},
   "outputs": [],
   "source": [
    "####Array to hold loss every 75 iterations \n",
    "Loss_AdaGrad = [0. for i in range(1500)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b95506",
   "metadata": {},
   "outputs": [],
   "source": [
    "#re-initialize weight matrices and bias terms\n",
    "W_ih = np.random.standard_normal(size=(hidden_neurons,sz_char))*.01 #connecting input to hidden layer\n",
    "W_hh =  np.random.standard_normal(size=(hidden_neurons,hidden_neurons))*.01 #connecting hidden to hidden layers\n",
    "W_ho = np.random.standard_normal(size=(sz_char,hidden_neurons))*.01 #connecting hidden to output layer\n",
    "b_h = np.zeros((hidden_neurons,1))\n",
    "b_o = np.zeros((sz_char,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08eafeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss at iteration 0 should be determined by size of vocabulary and the sequence size (number of times to unroll RNN)\n",
    "smooth_loss = -np.log(1/sz_char) * no_steps_unroll \n",
    "\n",
    "#Adagrad relies on memory (of previous observations) in addition to hidden states. Therefore we create memory versions of each model parameter\n",
    "W_ih_mem = np.zeros_like(W_ih)\n",
    "W_hh_mem = np.zeros_like(W_hh)\n",
    "W_ho_mem = np.zeros_like(W_ho)\n",
    "b_h_mem = np.zeros_like(b_h)\n",
    "b_o_mem = np.zeros_like(b_o)\n",
    "\n",
    "\n",
    "it = 0 #iteration number \n",
    "st = 0 #point at which to derive sample from data (data pointer)\n",
    "count_ind = 0 #counter for saving loss every 75th iteration \n",
    "\n",
    "#maximum number of iterations \n",
    "iter_ceiling = 200000000\n",
    "\n",
    "\n",
    "###Infinite loop. Stop when satisfied with sample\n",
    "#while True:\n",
    "while it <= 112500: \n",
    "    \n",
    "    #reset memory of RNN if we are at iteration 0 OR if we have reached end of data strand \n",
    "    if it == 0:\n",
    "        print(\"ITERATION 1\")\n",
    "        #initial hidden state to use after restarting \n",
    "        hidden_st = np.zeros((hidden_neurons,1))\n",
    "        #start back @ beginning of data\n",
    "        st = 0\n",
    "    elif no_steps_unroll + st + 1 >= sz_dat:\n",
    "        #initial hidden state to use after restarting \n",
    "        hidden_st = np.zeros((hidden_neurons,1))\n",
    "        #start back @ beginning of data \n",
    "        st =0                         \n",
    "    \n",
    "    #slicing data to match sequence size/number of unrolls \n",
    "    input_slice_char = all_poems[st:(st + no_steps_unroll)]\n",
    "    target_slice_char = all_poems[(st+1):(st+1+no_steps_unroll)]\n",
    "    \n",
    "    input_slice_int = np.zeros(len(input_slice_char),dtype=int)\n",
    "    target_slice_int = np.zeros(len(target_slice_char),dtype=int)\n",
    "\n",
    "    \n",
    "    #define these slices in terms of where characters appear in dictionary \n",
    "    for c in range(len(input_slice_char)):\n",
    "        input_slice_int[c] = characters.index(input_slice_char[c])\n",
    "        target_slice_int[c] = characters.index(target_slice_char[c])\n",
    "    \n",
    "    #check gradient estimates (eventually remove break)\n",
    "    #gradient_check(input_slice_int, target_slice_int, hidden_st)\n",
    "    #break\n",
    "    \n",
    "    #print out samples of model progress every 75 iterations \n",
    "    if it % 75 == 0:\n",
    "        print(\"POETRY SAMPLE\")\n",
    "        char_vec = np.zeros((sz_char,1))\n",
    "        #represents seed (start with current character)\n",
    "        char_vec[input_slice_int[0]] = 1\n",
    "        char_indices = []\n",
    "        h = np.copy(hidden_st)\n",
    "        for ch in range(300):\n",
    "            #forward pass through hidden layer\n",
    "            h = np.tanh(np.dot(W_ih,char_vec)+np.dot(W_hh,h)+ b_h) \n",
    "            out_unnorm = np.dot(W_ho,h) + b_o #unnormalized log probabilities \n",
    "            out_norm = np.exp(out_unnorm) / np.sum(np.exp(out_unnorm)) #normalized log probabilities \n",
    "            #selects one character randomly, however taking into account probabilities of each character \n",
    "            char_index = np.random.choice(range(sz_char),p=out_norm.ravel())\n",
    "            #one-hot vector representing selected character (for next iteration through for loop) \n",
    "            char_vec = np.zeros((sz_char,1))\n",
    "            char_vec[char_index] = 1\n",
    "            char_indices.append(char_index)\n",
    "        \n",
    "        #convert indices back to characters \n",
    "        sample = []\n",
    "        for c in range(len(char_indices)):\n",
    "            sample.append(characters[char_indices[c]])\n",
    "        #append characters into continuous string \n",
    "        sample_txt = ''.join(sample)\n",
    "        print('----\\n %s \\n----' % (sample_txt,))\n",
    "        \n",
    "        \n",
    "    \n",
    "    #calculate gradients from no_steps_unroll characters \n",
    "    dW_ih, dW_hh, dW_ho, d_b_hid, d_b_out, loss, hidden_st=loss_iterate(input_slice_int,target_slice_int,hidden_st,5)\n",
    "    \n",
    "    #smoothing loss so loss is averaged and not plotted/printed as erratic across iterations\n",
    "    smooth_loss = (smooth_loss * .999) + (loss * .001)\n",
    "    \n",
    "    #print loss every 75 iterations \n",
    "    if it % 75 == 0:\n",
    "        print(\"iteration: %d, loss: %f\" %(it,smooth_loss))\n",
    "        Loss_AdaGrad[count_ind] = smooth_loss\n",
    "        count_ind += 1\n",
    "    \n",
    "    \n",
    "    pad = 1e-8 ##To prevent dividing by zero\n",
    "    \n",
    "    for parameter, parameter_update, memory in zip([W_ih, W_hh, W_ho, b_h, b_o],\n",
    "                                                    [dW_ih,dW_hh,dW_ho,d_b_hid,d_b_out],\n",
    "                                                   [W_ih_mem,W_hh_mem,W_ho_mem,b_h_mem,b_o_mem]):\n",
    "        memory += parameter_update * parameter_update\n",
    "        parameter += -learning_rate * parameter_update / np.sqrt(memory + pad) \n",
    "\n",
    "    #increment iteration count by 1 and datapointer by size of unroll.\n",
    "    it = it + 1\n",
    "    st = st + no_steps_unroll\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fe7f40",
   "metadata": {},
   "source": [
    "## Part 5: Plotting loss for each algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b903d7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "epochs = range(1500)\n",
    "#epochs_to_iterations = range(0,112501,75)\n",
    "\n",
    "plt.plot(epochs, Loss_AdaGrad, label='AdaGrad')\n",
    "plt.plot(epochs, Loss_ADAM, label='ADAM')\n",
    "plt.plot(epochs, Loss_AMSGrad, label = 'AMSGrad')\n",
    "plt.title(\"Loss over time per optimization algorithm\")\n",
    "plt.xlabel(\"epoch (1 per 75 iterations)\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(8, 6, forward=True)\n",
    "plt.savefig(\"GDAlgorithmComparison_5.png\",format='png',dpi=200)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
